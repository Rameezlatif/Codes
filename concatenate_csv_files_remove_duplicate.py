# -*- coding: utf-8 -*-
"""Concatenate_CSV_files_remove_duplicate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rBhicvdyb9ofmEW_EFcR-lQ3CowWzQIU
"""

import pandas as pd

import pandas as pd

# List of CSV files to concatenate
csv_files = ["IRAK1_CHEMBL3357_05_bioactivity_data_2class_pIC50.csv", "IRAK4_CHEMBL3778_05_bioactivity_data_2class_pIC50.csv", "IRAK4_CHEMBL4523742_05_bioactivity_data_2class_pIC50.csv", "TAK1_CHEMBL5600_05_bioactivity_data_2class_pIC50.csv", "TAK1_CHEMBL5716_05_bioactivity_data_2class_pIC50.csv", "TAK1_CHEMBL5776_05_bioactivity_data_2class_pIC50.csv" , "TAK1_CHEMBL3038499_05_bioactivity_data_2class_pIC50.csv" , "TAK1_CHEMBL3885612_05_bioactivity_data_2class_pIC50.csv", "TRAF6_3883289_05_bioactivity_data_2class_pIC50.csv"]  # Replace with your file names

# Read and concatenate CSV files into a single DataFrame
dfs = [pd.read_csv(file) for file in csv_files]
concatenated_df = pd.concat(dfs, ignore_index=True)

# Find duplicate rows based on all columns
duplicates_df = concatenated_df[concatenated_df.duplicated(keep=False)]

# Remove duplicates from the concatenated DataFrame
unique_df = concatenated_df.drop_duplicates(keep=False)

# Write unique and duplicate DataFrames to separate CSV files
unique_df.to_csv("unique_output.csv", index=False)
duplicates_df.to_csv("duplicates_output.csv", index=False)

print("Unique content saved to 'unique_output.csv'")
print("Duplicated content saved to 'duplicates_output.csv'")